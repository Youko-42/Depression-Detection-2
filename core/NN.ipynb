{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NN.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyPeKzLne16fIkcSxMdYSEEg"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9MZIu3PbFx_u","executionInfo":{"status":"ok","timestamp":1607698049898,"user_tz":-480,"elapsed":592,"user":{"displayName":"Yangzi Zhong","photoUrl":"","userId":"14597158729399505185"}},"outputId":"baffa9b8-7534-4a5e-c312-2d63ebba7813"},"source":["import os\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n"," \n","path = \"/content/gdrive/My Drive/Independent\"\n","os.chdir(path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FurwtPnRa6ib","executionInfo":{"status":"ok","timestamp":1607698052966,"user_tz":-480,"elapsed":881,"user":{"displayName":"Yangzi Zhong","photoUrl":"","userId":"14597158729399505185"}},"outputId":"188ad64b-8987-48b2-971f-9e9e55804322"},"source":["%tensorflow_version 1.x"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hvkyu_tXGUY6","executionInfo":{"status":"ok","timestamp":1607698058785,"user_tz":-480,"elapsed":5300,"user":{"displayName":"Yangzi Zhong","photoUrl":"","userId":"14597158729399505185"}},"outputId":"08069b94-3230-4e97-9adc-721ef32234a3"},"source":["import numpy as np\n","import tensorflow as tf\n","from scipy import sparse\n","from keras import backend as K\n","from keras.models import Sequential\n","from sklearn.utils import class_weight\n","from keras.layers import Dense, Dropout\n","from keras.callbacks import EarlyStopping\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","\n","def save(users_posts, users_label):\n","    sparse.save_npz(\"./result/v_countposts.npz\", users_posts, False)\n","\n","    file = open(\"./result/v_label.txt\", \"w\")\n","    for user_label in users_label:\n","        file.write(str(user_label) + \"\\n\")\n","    file.close()\n","\n","def read_list(path):\n","    result = []\n","    with open(path, \"r\") as f:\n","        for line in f:\n","          temp = line.strip(\"\\n\")\n","          result.append(int(temp))\n","    return result\n","\n","def read_npz(path):\n","    sparse_matrix = sparse.load_npz(path)\n","    return sparse_matrix\n","\n","def recall_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall\n","\n","def precision_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","\n","def f1_m(y_true, y_pred):\n","    precision = precision_m(y_true, y_pred)\n","    recall = recall_m(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y1sMbl4CGXiP","executionInfo":{"status":"ok","timestamp":1607698837633,"user_tz":-480,"elapsed":273938,"user":{"displayName":"Yangzi Zhong","photoUrl":"","userId":"14597158729399505185"}},"outputId":"83b4f4b1-02e8-4855-93c0-4487ea3d9ea3"},"source":["# load dataset\n","X= read_npz(\"./result/t_tfidf_posts.npz\")\n","y= read_list(\"./result/t_label.txt\")\n","\n","scaler = StandardScaler()\n","X = X.toarray()\n","X = X[:, 50000:] # 0-50000为乱码\n","X = scaler.fit_transform(X)\n","n, d= X.shape\n","\n","precision_list = []\n","recall_list = []\n","f1_list = []\n","accuracy_list = []\n","\n","for i in range(1,10):\n","  # create model\n","  model = Sequential()\n","  model.add(Dense(d, input_dim=d, activation='relu', kernel_initializer='random_normal'))\n","  model.add(Dropout(0.1))\n","  model.add(Dense(64, activation='relu', kernel_initializer='random_normal'))\n","  model.add(Dense(16, activation='relu', kernel_initializer='random_normal'))\n","  model.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n","  model.compile(loss='binary_crossentropy', optimizer='RMSprop', metrics=['acc',f1_m,precision_m,recall_m])\n","\n","  K.get_session().run(tf.local_variables_initializer())\n","\n","  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n","\n","  # train model\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n","  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=i)\n","  history = model.fit(X_train, y_train, batch_size=40, epochs=10, verbose =0)\n","\n","  # evaluate the model\n","  loss, accuracy, f1, precision, recall = model.evaluate(X, y, verbose=0)\n","  precision_list.append(precision)\n","  recall_list.append(recall)\n","  f1_list.append(f1)\n","  accuracy_list.append(accuracy)\n","\n","print(np.mean(precision_list) , \" \" , np.std(precision_list))\n","print(np.mean(recall_list) , \" \" , np.std(recall_list))\n","print(np.mean(f1_list) , \" \" , np.std(f1_list))\n","print(np.mean(accuracy_list), \" \", np.std(accuracy_list))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.6964202920595804   0.0873192859800281\n","0.7350115842289395   0.07006654720636513\n","0.6872495611508688   0.06853493677450373\n","0.9595555663108826   0.01307339182893611\n"],"name":"stdout"}]}]}